# 自动点赞机器人

## 基本介绍

yaozi-real all rights reserved

可以自动为特定文章进行点赞，支持导出url的文本文档。专为聪明山设计，运行有效，操作方便。

## 使用教程

1. 将`searcher.py`的代码复制到一个新的文件中，在你的终端运行以下程序：
   `pip install selenium tqdm pandas requests`以下载所需的库。
2. 将文件末尾的`base_url`改为当下可访问的聪明山网址，`username="admin"`改为点赞所需的账号,`password="password"`为该账号密码。接着运行该文件。
   注意：需要有谷歌浏览器，账号需要是核心用户。
3. 运行结束后，在相同目录下会出现一个`admin_articles.txt`的文档。这里面存储着`所有文章的url`。文档内容以csv格式存储，里面有author与url两列，可以利用pandas找寻你想要的文章。
4. 将`likes_clicker.py`的代码复制到一个新的.py文件中，与修改后的`admin_articles.txt`放在相同目录下，运行该文件，就可以快速为你想要的文章点赞了。


## searcher.py

```
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import time
import pandas as pd

class BlogLikeBot:
    def __init__(self, base_url, username, password):
        self.base_url = base_url
        self.username = username
        self.password = password
        self.driver = webdriver.Chrome() 
        self.wait = WebDriverWait(self.driver, 10)
  
    def login(self):
        """登录网站"""
        try:
            self.driver.get(f"{self.base_url}/auth/login")
          
            username_input = self.wait.until(EC.presence_of_element_located((By.ID, "username")))
            password_input = self.driver.find_element(By.ID, "password")
            username_input.send_keys(self.username)
            password_input.send_keys(self.password)
            login_button = self.driver.find_element(By.ID, "submitBtn")
            login_button.click()
          
            self.wait.until(EC.url_contains("/"))
            print("登录成功！")
            return True

        except TimeoutException:
            print("登录失败或超时")
            return False
  
    def navigate_to_blog(self):
        """导航到博客区"""
        try:
            time.sleep(2)
            blog_link = self.wait.until(
                EC.element_to_be_clickable((By.LINK_TEXT, "博客"))
            )
            blog_link.click()

            # 等待博客列表加载
            self.wait.until(EC.url_contains("/blog"))
            print("成功进入博客区!")
            return True
          
        except TimeoutException:
            print("无法进入博客区")
            return False
  
    def find_admin_articles(self):
        """查找文章"""
        try:

            time.sleep(1)
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
          
            articles = []
            elements = self.driver.find_elements(By.TAG_NAME, "article")
            if elements:
                articles = elements
                print(f"找到 {len(articles)} 个文章元素")
            else:
                print(f"未找到文章")
          
            # 处理找到的文章
            print(f"\n开始处理 {len(articles)} 篇文章...")
            admin_articles = []
          
            for i, article in enumerate(articles):
                try:
                    # 寻找文章作者
                    author = None
                    author_element = article.find_element(By.CSS_SELECTOR, "[class*='author']")
                    author = author_element.text.strip()
                    
                    # 处理作者格式
                    if author:
                        author = author_element.text.strip().split('\n')[0]
                        link = article.find_element(By.TAG_NAME, "a")
                        url = link.get_attribute("href")
                        admin_articles.append([author,url])
                      
                except Exception as e:
                    print(f" 处理文章 {i+1} 时出错: {e}")
                    continue
          
            print(f"\n最终找到 {len(admin_articles)} 篇文章")
            return admin_articles
          
        except Exception as e:
            print(f"查找文章时出错: {e}")
            import traceback
            traceback.print_exc()
            return []

    def save_articles_to_file(self, articles, filename="admin_articles.txt"):
        """保存为csv文件"""
        try:
            df = pd.DataFrame(articles, columns=['作者', '文章链接'])
            df = df.sort_values(by='作者', ascending=True)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"已保存 {len(articles)} 篇文章到 {filename}")
            return True
        except Exception as e:
            print(f"保存失败: {e}")
            return False

    def run(self):
        """执行完整流程"""
        try:
            # 登录
            if not self.login():
                return
          
            # 进入博客区
            if not self.navigate_to_blog():
                return
          
            # 查找文章
            admin_articles = self.find_admin_articles()
          
            # 保存文章链接到文件
            self.save_articles_to_file(admin_articles)
          
        finally:
            # 关闭浏览器
            time.sleep(3)
            self.driver.quit()

if __name__ == "__main__":
    bot = BlogLikeBot(
        base_url="base_url",
        username="admin",
        password="password",
    )
    bot.run()
```

## likes_clicker.py

```
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.exceptions import Timeout, ConnectionError, ProxyError, RequestException
import time
from tqdm import tqdm

class ProgressBarBlogBot:
    def __init__(self, base_url, username, password):
        self.base_url = base_url.rstrip('/')
        self.username = username
        self.password = password
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        })

    def login_and_get_cookies(self):
        """Selenium 登录并获取 Cookie"""

        print(">>> 正在启动浏览器进行登录...")
        options = webdriver.ChromeOptions()
        options.add_argument("--headless") 
        options.add_argument("--disable-gpu")
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        
        driver = webdriver.Chrome(options=options)
        wait = WebDriverWait(driver, 10)

        try:
            driver.get(f"{self.base_url}/auth/login")
            wait.until(EC.presence_of_element_located((By.ID, "username"))).send_keys(self.username)
            driver.find_element(By.ID, "password").send_keys(self.password)
            driver.find_element(By.ID, "submitBtn").click()
            wait.until(EC.url_contains("/"))
            time.sleep(1) 
            
            selenium_cookies = driver.get_cookies()
            for cookie in selenium_cookies:
                self.session.cookies.set(cookie['name'], cookie['value'])
            
            print(">>> 登录成功，Cookie 已提取。\n")
            return True
        except Exception as e:
            print(f"!!! 登录失败: {e}")
            return False
        finally:
            driver.quit()

    def load_articles_from_file(self, filename="admin_articles.txt"):
        """从文件加载文章 URL"""

        try:
            lines = []
            with open(filename, 'r', encoding='utf-8') as f:
                next(f) # 跳过表头
                for line in f:
                    line = line.strip()
                    char = 0
                    while char < len(line):
                        if line[char] == ',':
                            line = line[char+1:]
                        char += 1
                    lines.append(line)
            return lines              

        except FileNotFoundError:
            return []

    def like_article_api(self, article_url):
        """
        核心请求逻辑
        返回: (是否成功, url, 是否需要重试)
        """
        article_url = article_url.rstrip('/')
        target_url = f"{article_url}/like"
        headers = {"Referer": article_url}
        
        try:
            # 设置较短的超时，配合多轮清洗
            response = self.session.post(target_url, headers=headers, timeout=5)
            
            if response.status_code == 200:
                return True, article_url, False
            elif response.status_code in [500, 502, 503, 504]:
                return False, article_url, True # 服务器错误，重试
            else:
                return False, article_url, False # 403/404 等，不重试

        except (Timeout, ConnectionError, ProxyError):
            return False, article_url, True # 网络错误，重试
        except RequestException:
            return False, article_url, False

    def run(self):
        '''主运行函数'''

        start_time = time.time()
        if not self.login_and_get_cookies():
            return

        pending_articles = self.load_articles_from_file()
        total_initial = len(pending_articles)
        if total_initial == 0: 
            print("没有读取到文章。")
            return

        max_rounds = 5  # 最大清洗轮数
        success_total = 0
        
        # 主循环
        for round_idx in range(1, max_rounds + 1):
            if not pending_articles:
                break 
            
            current_total = len(pending_articles)
            retry_list = [] 
            
            # 本轮统计
            round_success = 0
            round_fail = 0
            print(f">>> 第 {round_idx}/{max_rounds} 轮清洗开始 (待处理: {current_total}) ---")
            
            # 开启线程池
            with ThreadPoolExecutor(max_workers = 20) as executor:
                # 提交任务
                future_to_url = {executor.submit(self.like_article_api, url): url for url in pending_articles}
                with tqdm(total=current_total, unit='篇', desc='进度', ncols=100, colour='green') as pbar: 
                    for future in as_completed(future_to_url):
                        is_success, url, should_retry = future.result()
                        if is_success:
                            success_total += 1
                            round_success += 1
                        else:
                            round_fail += 1
                            if should_retry:
                                retry_list.append(url)
                        
                        # 更新进度条
                        pbar.update(1)
                        pbar.set_postfix(成功=round_success, 失败=round_fail, 待重试=len(retry_list))

            # 更新待处理列表
            pending_articles = retry_list
            if pending_articles:
                time.sleep(1)
                
        # 计算耗时
        end_time = time.time()
        total_time = end_time - start_time

        # 最终报告
        print(f" 最终任务报告")
        print(f" 总文章数 : {total_initial}")
        print(f" 成功点赞 : {success_total}")
        print(f" 成功率 : {success_total / total_initial:.2%}")
        print(f" 耗时 : {total_time:.2f} 秒")
        
        if pending_articles:
            print(f" 有 {len(pending_articles)} 篇失败，已保存至 failed_urls.txt")
            with open("failed_urls.txt", "w", encoding="utf-8") as f:
                for url in pending_articles:
                    f.write(url + "\n")
        else:
            print(" 所有文章处理完毕！")

if __name__ == "__main__":
    bot = ProgressBarBlogBot(
        base_url="base_url",
        username="admin",
        password="password",
    )
    bot.run()
```

